Finite Markov Decision Processes: Finite MDPs



===== MDP框架的灵活多样型：我们通常会把面对的强化学习问题构建进MDP框架：

The MDP framework is abstract and flexible and can be applied to many different problems in many different ways. 

For example, the time steps need not refer to fixed intervals of real time; they can refer to arbitrary successive stages of decision making and acting.

The actions can be low-level controls, such as the voltages applied to the motors of a robot arm, or high-level decisions, such as whether or not to have lunch or to go to graduate school. {MDP中的actions可以是low-level controls比如机械臂中的电压大小，也可以是high-level decisions比如我们是否要去读研}

Similarly, the states can take a wide variety of forms. They can be completely determined by low-level sensations, such as direct sensor readings, or they can be more high-level and abstract, such as symbolic descriptions of objects in a room.
{state也可以是多种多样的，它可以是low-level sensation signals，比如agent上的sensor感应到的数据；它也可以是抽象的high-level的，比如它可以是房间内objects' abstract representations}

Some of what makes up a state could be based on memory of past sensations or even be entirely mental or subjective. For example, an agent could be in the state of not being sure where an object is, or of having just been surprised in some clear defined sense.

In particular, the boundary between agent and environment is typically not the same as the physical boundary of a robot's. Usually, the boundary is drawn closer to the agent than that. For example, the motors and mechanical linkages of a robot 



Example 3.3: Recycling Robot

A mobile robot has the job of collecting empty soda cans in an office environment. It has sensors for detecting cans, and an arm and gripper that can pick them up and place them in an onboard bin; it runs on a rechargeable battery. The robot's control system has components for interpreting sensory information, for navigating, and for controlling the arm and gripper. High-level decisions about how to search for cans are made by a reinforcement learning agent based on the cuurent charge level of the battery. To make a simple example, we assume that only two charge levels can be distuiguished, comprising a small state set 


3.2 Goals and Rewards

  In reinforcement learning, the purpose of goal of the agent is formalized in terms of a special signal, called the reward, passing from the environment to the agent. At each time step, the reward is a simple number, R_t belongs to real number. Informally, the agent's goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the reward hypothesis:
  " That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)."
  {agent的目标就是去maximize the expected cumulative sum of rewards}
  
  The use of a reward signal to formalize the idea of a goal is one of the most distinctive features of reinforcement learning.
  
  Although formulating goals in terms of reward signals might at first appear limiting, in practice it has proved to be flexible and widely applicable. The best way to see this is to consider examples of how it has been, or could be, used. For example, to make a robot learn to walk, researches have provided reward on each time step propotional to the robot's forward motion. In making a robot learn how to escape from a maze, the reward is often -1 for every time step that passes prior to escape; this encourages the agent to escape as quickly as possible. To make a robot learn to find and collect empty soda cans for recycling, one might give it a reward of zero
  
  !!!! {You can see what is happening in all of these examples. The agent always learns to maximize its reward. If we want it to do something for us, we must provide rewards to it in such a way that in maximizing them the agent will also achieve our goals. It is thus critical that the rewards we set up truly indicate what we want accomplished. In particular, the reward siginal is not the place to impart to the agent prior knowledge about "how to achieve" what we want it to do. For example, a chess-playing agent should be rewarded only for actually winning, not for achieving subgoals such as taking its opponent's pieces or wining control of the center of the board. If achieving these sorts of subgoals were rewarded, then the agent might find a way to achieve them without achieving the real goal. For example ,it might find a way to take the opponent's pieces even at the cost of losing the game. The reward signal is your way of communicating to the agent what you want achieved, not how you want it achieved. agent需要在最大化reward的过程中，自己摸索出实现终极目标的方案。我们要合理地设置reward，并给出所有可选的actions。 }
  

3.3 Returns and Episodes:

  So far we have discussed informally the objective of learning. We have said that the agent's goal is to maximize the cumulative reward it receives in the long run. How might this be defined formally? If the sequence of rewards received after time step t is denoted as R_t+1, R_t+2, R_t+3, ..., then what precise aspect of this sequence do we wish to maximize?

  G_t = R_t+1 + R_t+2 + R_t+3 + ... + R_T, where T is a final time step. This approach makes sense in applications in which there is a natural notion of final time step, that is, when the agent-environment interation breaks naturally into subsequences, which we call episodes, such as plays of a gam, trips through a maze, or any sort of repeated interation. Each episode ends in a special state called the terminal state,


